{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34eef629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model type: <class 'sklearn.pipeline.Pipeline'>\n",
      "[INFO] Pipeline steps:\n",
      "  - scaler -> <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "  - clf -> <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "[INFO] Scaler expects n_features = 5\n",
      "[INFO] Inferred n_features = 5\n",
      "[INFO] Attempting to convert using inferred n_features...\n",
      "[INFO] Using initial_type with n_features = 5\n",
      "[OK] Saved ONNX to: model/landslide_rf_pipeline.onnx\n"
     ]
    }
   ],
   "source": [
    "# inspect_and_convert_to_onnx.py\n",
    "import joblib\n",
    "import numpy as np\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import os, sys, traceback\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "\n",
    "model_path = \"model/landsilde_rf_pipeline_best.joblib\"\n",
    "target_opset = 11\n",
    "\n",
    "def find_pipeline(est):\n",
    "    # ถ้าเป็น RandomizedSearchCV หรือ GridSearchCV ให้ใช้ best_estimator_\n",
    "    if hasattr(est, \"best_estimator_\"):\n",
    "        print(\"[INFO] Using .best_estimator_ from search object\")\n",
    "        est = est.best_estimator_\n",
    "    return est\n",
    "\n",
    "def try_get_steps(est):\n",
    "    steps = getattr(est, \"steps\", None)\n",
    "    if steps:\n",
    "        print(\"[INFO] Pipeline steps:\")\n",
    "        for n, s in steps:\n",
    "            print(\"  -\", n, \"->\", type(s))\n",
    "    else:\n",
    "        print(\"[INFO] No steps attribute on model (not a pipeline?)\")\n",
    "    return steps\n",
    "\n",
    "def find_scaler_feature_count(est):\n",
    "    # ถ้ามี named_steps และ scaler อยู่ ให้ดึง n_features_in_ หรือ mean_.shape\n",
    "    try:\n",
    "        named = getattr(est, \"named_steps\", None)\n",
    "        if named:\n",
    "            # ลองหา scaler-like step (ชื่อ 'scaler' หรือ object type StandardScaler)\n",
    "            if 'scaler' in named:\n",
    "                sc = named['scaler']\n",
    "            else:\n",
    "                # search for any StandardScaler\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                sc = None\n",
    "                for k, v in named.items():\n",
    "                    if isinstance(v, StandardScaler):\n",
    "                        sc = v\n",
    "                        print(f\"[INFO] Found StandardScaler at step '{k}'\")\n",
    "                        break\n",
    "                if sc is None:\n",
    "                    print(\"[INFO] No StandardScaler found in named_steps\")\n",
    "                    return None\n",
    "            # now inspect scaler\n",
    "            n = getattr(sc, \"n_features_in_\", None)\n",
    "            if n is None:\n",
    "                mean = getattr(sc, \"mean_\", None)\n",
    "                if mean is not None:\n",
    "                    n = getattr(mean, \"shape\", [None])[0]\n",
    "            print(\"[INFO] Scaler expects n_features =\", n)\n",
    "            return int(n) if n is not None else None\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Error while inspecting scaler:\", e)\n",
    "    # fallback: try estimator level attribute\n",
    "    n = getattr(est, \"n_features_in_\", None)\n",
    "    if n is not None:\n",
    "        print(\"[INFO] Estimator n_features_in_ =\", n)\n",
    "        return int(n)\n",
    "    return None\n",
    "\n",
    "def remove_smote_if_any(est):\n",
    "    # If pipeline has 'smote' step (imblearn), remove it for conversion\n",
    "    steps = getattr(est, \"steps\", None)\n",
    "    if not steps:\n",
    "        return est\n",
    "    filtered = [(n, s) for (n, s) in steps if n.lower() != 'smote']\n",
    "    if len(filtered) != len(steps):\n",
    "        print(\"[INFO] Removed 'smote' step from pipeline for conversion\")\n",
    "        return SkPipeline(filtered)\n",
    "    return est\n",
    "\n",
    "def convert_with_inferred_dim(est, n_features):\n",
    "    if n_features is None:\n",
    "        raise ValueError(\"n_features is None - cannot set initial_type automatically\")\n",
    "    initial_type = [('float_input', FloatTensorType([None, int(n_features)]))]\n",
    "    print(\"[INFO] Using initial_type with n_features =\", n_features)\n",
    "    onx = convert_sklearn(est, initial_types=initial_type, target_opset=target_opset)\n",
    "    return onx\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Could not load model:\", e)\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Loaded model type:\", type(model))\n",
    "\n",
    "    # Use best_estimator_ if wrapper\n",
    "    model = find_pipeline(model)\n",
    "\n",
    "    # show pipeline steps if any\n",
    "    try_get_steps(model)\n",
    "\n",
    "    # Possibly remove SMOTE if present\n",
    "    model_for_export = remove_smote_if_any(model)\n",
    "\n",
    "    # Try to infer n_features from scaler or estimator\n",
    "    n_features = find_scaler_feature_count(model_for_export)\n",
    "    print(\"[INFO] Inferred n_features =\", n_features)\n",
    "\n",
    "    # If inferred n_features is small (e.g. 6) we proceed; if it's large (e.g. 400)\n",
    "    # we warn user to double-check that this is the expected model.\n",
    "    if n_features is not None:\n",
    "        print(\"[INFO] Attempting to convert using inferred n_features...\")\n",
    "        try:\n",
    "            onx = convert_with_inferred_dim(model_for_export, n_features)\n",
    "            out_path = \"model/landslide_rf_pipeline.onnx\"\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(onx.SerializeToString())\n",
    "            print(\"[OK] Saved ONNX to:\", out_path)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Conversion failed with inferred n_features:\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # If we get here, either we couldn't infer or conversion failed\n",
    "    print(\"\"\"\n",
    "[FAILED] Could not convert automatically.\n",
    "Diagnostics you should check:\n",
    "  * Are you converting the correct model file? (maybe the model was trained on many features)\n",
    "  * Print model.named_steps and inspect which transformer expands features (OneHotEncoder/ColumnTransformer)\n",
    "  * If scaler expects 400 features but you intend to use 6 features, you're converting the wrong model.\n",
    "  * You can manually set initial_type = [('float_input', FloatTensorType([None, 6]))] but ensure pipeline expects 6 inputs.\n",
    "\"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba36aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
