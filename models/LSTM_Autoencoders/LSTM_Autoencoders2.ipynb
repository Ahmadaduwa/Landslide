{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0dc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas matplotlib scikit-learn tensorflow seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a540fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv(\"../../data/dev101_prepared.csv\")\n",
    "#df2 = pd.read_csv(\"../../data/dev102_prepared.csv\")\n",
    "\n",
    "#df_all = pd.concat([df1, df2], ignore_index=True)\n",
    "df_all = pd.read_csv(\"../../data/devAll2_prepared.csv\")\n",
    "\n",
    "# แปลง timestamp และเรียงลำดับ\n",
    "df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\n",
    "df_all = df_all.sort_values(by='timestamp').reset_index(drop=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['devID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da59da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature ที่จะใช้\n",
    "numerical_cols = ['soil', 'rain', 'temp', 'humi', 'geo', 'lat', 'lng']\n",
    "\n",
    "# สร้าง Scaler และฟิตข้อมูล\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(df_all[numerical_cols])\n",
    "\n",
    "# เพิ่มคอลัมน์ scaled กลับเข้า DataFrame\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    df_all[f'scaled_{col}'] = scaled_values[:, i]\n",
    "\n",
    "# Save scaler (สำหรับใช้งานจริงในอนาคต)\n",
    "joblib.dump(scaler, 'scaler2.save')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "def create_sequences_by_device(df, sequence_length, numerical_cols):\n",
    "    sequences = []\n",
    "    device_ids = []\n",
    "    timestamps = []\n",
    "\n",
    "    for dev_id, group in df.groupby('devID'):\n",
    "        group = group.sort_values('timestamp')\n",
    "        values = group[[f'scaled_{col}' for col in numerical_cols]].values\n",
    "        time_vals = group['timestamp'].values\n",
    "\n",
    "        for i in range(len(values) - sequence_length + 1):\n",
    "            sequences.append(values[i:i + sequence_length])\n",
    "            device_ids.append(dev_id)\n",
    "            timestamps.append(time_vals[i + sequence_length - 1])  \n",
    "\n",
    "    return np.array(sequences), device_ids, timestamps\n",
    "\n",
    "X_seq, device_ids, seq_timestamps = create_sequences_by_device(df_all, SEQUENCE_LENGTH, numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_seq)\n",
    "train_size = int(n * 0.7)\n",
    "val_size = int(n * 0.2)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "X_train = X_seq[:train_size]\n",
    "X_val = X_seq[train_size:train_size + val_size]\n",
    "X_test = X_seq[train_size + val_size:]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(numerical_cols)\n",
    "input_shape = (SEQUENCE_LENGTH, n_features)\n",
    "\n",
    "encoder_inputs = Input(shape=input_shape)\n",
    "encoded = LSTM(32, activation='relu', return_sequences=False)(encoder_inputs)\n",
    "decoded = RepeatVector(SEQUENCE_LENGTH)(encoded)\n",
    "decoded = LSTM(32, activation='relu', return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(n_features))(decoded)\n",
    "\n",
    "model = Model(encoder_inputs, decoded)\n",
    "model.compile(optimizer=Adam(0.001), loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, X_train,\n",
    "    validation_data=(X_val, X_val),\n",
    "    epochs=5, #90\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('lstm_autoencoder_model2.h5')\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss During Training')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set\n",
    "X_test_pred = model.predict(X_test)\n",
    "test_reconstruction_error = np.mean(np.mean(np.square(X_test - X_test_pred), axis=1), axis=1)\n",
    "\n",
    "# แสดงค่า loss บน test set\n",
    "test_loss = np.mean(test_reconstruction_error)\n",
    "print(f\"Test Reconstruction Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40afb45",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff702b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_test = pd.read_csv('../../data/devAll2_prepared.csv')\n",
    "df_original_features = df_test.copy() \n",
    "\n",
    "df_test['timestamp'] = pd.to_datetime(df_test['timestamp'])\n",
    "df_test = df_test.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "scaler = joblib.load('scaler2.save')\n",
    "numerical_cols = ['soil', 'rain', 'temp', 'humi', 'geo', 'lat', 'lng']\n",
    "\n",
    "missing_cols = [col for col in numerical_cols if col not in df_test.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in input CSV: {missing_cols}\")\n",
    "\n",
    "scaled_values = scaler.transform(df_test[numerical_cols])\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    df_test[f'scaled_{col}'] = scaled_values[:, i]\n",
    "\n",
    "# === sequence for predict ===\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "def create_sequences(df, sequence_length, numerical_cols):\n",
    "    sequences, device_ids, timestamps = [], [], []\n",
    "    # เก็บ index เริ่มต้นของแต่ละ sequence เพื่อเชื่อมโยงกับ df_test\n",
    "    start_indices = []\n",
    "\n",
    "    for dev_id, group in df.groupby('devID'):\n",
    "        group = group.sort_values('timestamp')\n",
    "        values = group[[f'scaled_{col}' for col in numerical_cols]].values\n",
    "        time_vals = group['timestamp'].values\n",
    "        \n",
    "        # เพิ่มคอลัมน์ original_index เพื่อให้สามารถเชื่อมโยงกลับไปยัง df_test ได้\n",
    "        group = group.reset_index(drop=False).rename(columns={'index': 'original_df_index'})\n",
    "\n",
    "        for i in range(len(values) - sequence_length + 1):\n",
    "            sequences.append(values[i:i + sequence_length])\n",
    "            device_ids.append(dev_id)\n",
    "            timestamps.append(time_vals[i + sequence_length - 1]) # timestamp ของจุดสุดท้ายใน sequence\n",
    "            start_indices.append(group['original_df_index'].iloc[i + sequence_length - 1]) # เก็บ original index ของจุดสุดท้าย\n",
    "\n",
    "    return np.array(sequences), device_ids, timestamps, start_indices\n",
    "\n",
    "# สร้าง sequence\n",
    "X_seq_test, device_ids_test, timestamps_test, start_indices_test = create_sequences(df_test, SEQUENCE_LENGTH, numerical_cols)\n",
    "\n",
    "# === 5. โหลดโมเดลและ predict ===\n",
    "model = load_model('lstm_autoencoder_model2.h5', compile=False)\n",
    "X_pred = model.predict(X_seq_test)\n",
    "\n",
    "# === 6. คำนวณ reconstruction error ===\n",
    "reconstruction_error = np.mean(np.mean(np.square(X_seq_test - X_pred), axis=1), axis=1)\n",
    "\n",
    "# === 7. สร้าง DataFrame เก็บผล ===\n",
    "df_results = pd.DataFrame({\n",
    "    'devID': device_ids_test,\n",
    "    'timestamp': timestamps_test,\n",
    "    'reconstruction_error': reconstruction_error,\n",
    "    'original_df_index': start_indices_test # เพิ่มคอลัมน์นี้เพื่อ merge กับ df_original_features\n",
    "})\n",
    "\n",
    "# === 8. คำนวณ thresholds และกำหนด Anomaly Level ===\n",
    "threshold_warning = 0.01\n",
    "threshold_critical = 0.0225\n",
    "\n",
    "def classify_anomaly(err):\n",
    "    if err > threshold_critical:\n",
    "        return 'critical'\n",
    "    elif err > threshold_warning:\n",
    "        return 'warning'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "df_results['anomaly_level'] = df_results['reconstruction_error'].apply(classify_anomaly)\n",
    "\n",
    "print(f\"95th percentile (Warning) threshold = {threshold_warning:.5f}\")\n",
    "print(f\"99th percentile (Critical) threshold = {threshold_critical:.5f}\")\n",
    "print(\"\\n--- Anomaly Level Counts ---\")\n",
    "print(df_results['anomaly_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3566e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- การรวมข้อมูลเพื่อการวิเคราะห์เพิ่มเติม ---\n",
    "# เราจะ merge df_results กับ df_original_features โดยใช้ 'original_df_index'\n",
    "# เพื่อให้มีคอลัมน์ features เดิมที่ไม่ถูก scale อยู่ใน DataFrame เดียวกัน\n",
    "# สร้างคอลัมน์ 'index' ใน df_original_features ให้ตรงกับ 'original_df_index'\n",
    "# Remove duplicate columns in df_original_features (keep only one 'original_df_index')\n",
    "df_original_features = df_original_features.loc[:, ~df_original_features.columns.duplicated()]\n",
    "\n",
    "# Ensure 'original_df_index' exists and is unique\n",
    "if 'original_df_index' not in df_original_features.columns:\n",
    "    df_original_features = df_original_features.reset_index().rename(columns={'index': 'original_df_index'})\n",
    "\n",
    "# Convert timestamp to datetime64[ns] for merge compatibility\n",
    "df_original_features['timestamp'] = pd.to_datetime(df_original_features['timestamp'])\n",
    "\n",
    "# Merge ข้อมูล\n",
    "df_analysis = pd.merge(df_results, df_original_features, on=['devID', 'timestamp', 'original_df_index'], how='left')\n",
    "\n",
    "# ตรวจสอบว่ามีค่า NaN หลังจาก Merge หรือไม่ (ถ้ามี แสดงว่า timestamp/devID ไม่ตรงกันพอดี)\n",
    "if df_analysis.isnull().any().any():\n",
    "    print(\"\\nWarning: Missing values found in merged DataFrame. Ensure timestamps and devIDs match for accurate feature analysis.\")\n",
    "\n",
    "# --- เริ่มการสร้าง Visualization ---\n",
    "# ตั้งค่า palette สำหรับ Anomaly Levels\n",
    "palette_anomaly = {'normal': 'green', 'warning': 'orange', 'critical': 'red'}\n",
    "\n",
    "# --- 1. Distribution of Reconstruction Error ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_results['reconstruction_error'], bins=200, kde=True, color='purple', label='Reconstruction Error')\n",
    "plt.axvline(threshold_warning, color='orange', linestyle='--', label=f'Warning Threshold ({threshold_warning:.5f})')\n",
    "plt.axvline(threshold_critical, color='red', linestyle='--', label=f'Critical Threshold ({threshold_critical:.5f})')\n",
    "plt.title('Distribution of Reconstruction Error (Anomaly Score from Autoencoder)')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944683cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Anomaly Level Counts ---\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.countplot(data=df_results, x='anomaly_level', order=['normal', 'warning', 'critical'], palette=palette_anomaly)\n",
    "plt.title('Amount of Data in Each Anomaly Level')\n",
    "plt.xlabel('Anomaly Level')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Reconstruction Error Over Time with Anomaly Levels (All Devices) ---\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.scatterplot(\n",
    "    data=df_results, x='timestamp', y='reconstruction_error',\n",
    "    hue='anomaly_level', palette=palette_anomaly, alpha=0.7, s=15,\n",
    "    hue_order=['normal', 'warning', 'critical']\n",
    ")\n",
    "plt.axhline(threshold_warning, color='orange', linestyle='--', label=f'Warning Threshold ({threshold_warning:.5f})')\n",
    "plt.axhline(threshold_critical, color='red', linestyle='--', label=f'Critical Threshold ({threshold_critical:.5f})')\n",
    "plt.title('Reconstruction Error over Time with Anomaly Levels (All Device IDs)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Anomaly Level')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.scatterplot(\n",
    "    data=df_results[df_results['devID'] == 101.0], \n",
    "    x='timestamp', y='reconstruction_error',\n",
    "    hue='anomaly_level', palette=palette_anomaly, alpha=0.7, s=15,\n",
    "    hue_order=['normal', 'warning', 'critical']\n",
    ")\n",
    "plt.axhline(threshold_warning, color='orange', linestyle='--', label=f'Warning Threshold ({threshold_warning:.5f})')\n",
    "plt.axhline(threshold_critical, color='red', linestyle='--', label=f'Critical Threshold ({threshold_critical:.5f})')\n",
    "plt.title('Reconstruction Error over Time with Anomaly Levels (Device 101)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Anomaly Level')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.scatterplot(\n",
    "    data=df_results[df_results['devID'] == 102.0], \n",
    "    x='timestamp', y='reconstruction_error',\n",
    "    hue='anomaly_level', palette=palette_anomaly, alpha=0.7, s=15,\n",
    "    hue_order=['normal', 'warning', 'critical']\n",
    ")\n",
    "plt.axhline(threshold_warning, color='orange', linestyle='--', label=f'Warning Threshold ({threshold_warning:.5f})')\n",
    "plt.axhline(threshold_critical, color='red', linestyle='--', label=f'Critical Threshold ({threshold_critical:.5f})')\n",
    "plt.title('Reconstruction Error over Time with Anomaly Levels (Device 101)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Anomaly Level')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Box Plot of Original Features by Anomaly Level ---\n",
    "print(\"\\n--- Box Plots of Original Features by Anomaly Level ---\")\n",
    "# Optional: define boxprops if you want to customize box appearance\n",
    "# boxprops = dict(edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for feature in numerical_cols: # ใช้ numerical_cols เพื่อเข้าถึงคอลัมน์ที่ไม่ถูก scale\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.boxplot(\n",
    "        data=df_analysis,\n",
    "        x='anomaly_level',\n",
    "        y=feature,\n",
    "        order=['normal', 'warning', 'critical'],\n",
    "        palette=palette_anomaly\n",
    "    )\n",
    "    plt.title(f'Boxplot of {feature} by Anomaly Level', fontsize=14)\n",
    "    plt.xlabel('Anomaly Level', fontsize=12)\n",
    "    plt.ylabel(feature, fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nAll Box Plots for Original Features successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ea369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1. Bar Plot: จำนวนข้อมูลแต่ละ devID ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "df_results['devID'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Amount of devID in Sequence')\n",
    "plt.xlabel('devID')\n",
    "plt.ylabel('จำนวน Sequence')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 5.2. Dynamic Thresholds (Percentile) ---\n",
    "threshold_warning = np.percentile(df_results['reconstruction_error'], 95)\n",
    "threshold_critical = np.percentile(df_results['reconstruction_error'], 99)\n",
    "print(f\"Dynamic 95th percentile (Warning) threshold = {threshold_warning:.5f}\")\n",
    "print(f\"Dynamic 99th percentile (Critical) threshold = {threshold_critical:.5f}\")\n",
    "\n",
    "# --- 5.3. PCA Plot of Last Sequence Features by Anomaly Level ---\n",
    "# X_seq_test มี shape (num_sequences, SEQUENCE_LENGTH, num_features)\n",
    "# เราจะใช้ค่าจาก timestamp สุดท้าย (index SEQUENCE_LENGTH - 1)\n",
    "X_features_last_step = X_seq_test[:, -1, :] # นี่คือ features ที่ scaled แล้วของ timestamp สุดท้ายในแต่ละ sequence\n",
    "\n",
    "pca_autoencoder = PCA(n_components=2)\n",
    "X_pca_autoencoder = pca_autoencoder.fit_transform(X_features_last_step)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=X_pca_autoencoder[:, 0], y=X_pca_autoencoder[:, 1],\n",
    "                hue=df_results['anomaly_level'], style=df_results['devID'],\n",
    "                palette=palette_anomaly, alpha=0.7, s=30,\n",
    "                hue_order=['normal', 'warning', 'critical'])\n",
    "plt.title('PCA Plot of Last Sequence Features\\nColored by Anomaly Level & Device ID (Autoencoder)')\n",
    "plt.xlabel(f'Principal Component 1 ({pca_autoencoder.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca_autoencoder.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "plt.legend(title='Anomaly Level / devID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBar Plot of devID count, dynamic thresholds, and PCA Plot successfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01386b",
   "metadata": {},
   "source": [
    "# Sumulate in Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6aa090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import collections # สำหรับ deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "scaler = joblib.load('scaler2.save')\n",
    "model = load_model('lstm_autoencoder_model2.h5', compile=False)\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "numerical_cols = ['soil', 'rain', 'temp', 'humi', 'geo', 'lat', 'lng']\n",
    "LAST_N_ROWS = 1000\n",
    "\n",
    "df_full_data_source = pd.read_csv('../../data/devAll2_prepared.csv')\n",
    "df_full_data_source['timestamp'] = pd.to_datetime(df_full_data_source['timestamp'])\n",
    "\n",
    "df_full_data_source = df_full_data_source.sort_values(by=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "df_stream_data_source = df_full_data_source.tail(LAST_N_ROWS).reset_index(drop=True)\n",
    "\n",
    "# --- hresholds ---\n",
    "threshold_warning = 0.01\n",
    "threshold_critical = 0.02\n",
    "\n",
    "def classify_anomaly(err):\n",
    "    if err > threshold_critical:\n",
    "        return 'critical'\n",
    "    elif err > threshold_warning:\n",
    "        return 'warning'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "# --- การเตรียมบัฟเฟอร์สำหรับแต่ละ devID ---\n",
    "device_buffers = {} \n",
    "device_results = collections.defaultdict(list) \n",
    "\n",
    "print(f\"--- Starting Real-time Anomaly Detection Simulation (Last {LAST_N_ROWS} Rows by Timestamp) ---\")\n",
    "print(f\"Warning Threshold: {threshold_warning:.5f}\")\n",
    "print(f\"Critical Threshold: {threshold_critical:.5f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# ลูปผ่านข้อมูลเพื่อจำลองการรับค่าทีละแถว\n",
    "current_dev_id_processing = None\n",
    "\n",
    "for index, row in df_stream_data_source.iterrows():\n",
    "    dev_id = row['devID']\n",
    "    current_timestamp = row['timestamp']\n",
    "    current_features = row[numerical_cols].values \n",
    "    \n",
    "    if dev_id not in device_buffers:\n",
    "        device_buffers[dev_id] = collections.deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    scaled_features = scaler.transform(current_features.reshape(1, -1))[0]\n",
    "    \n",
    "    device_buffers[dev_id].append(scaled_features)\n",
    "    \n",
    "    if len(device_buffers[dev_id]) == SEQUENCE_LENGTH:\n",
    "        input_sequence = np.array(device_buffers[dev_id]).reshape(1, SEQUENCE_LENGTH, len(numerical_cols))\n",
    "\n",
    "        predicted_sequence = model.predict(input_sequence, verbose=0)\n",
    "\n",
    "        error = np.mean(np.mean(np.square(input_sequence - predicted_sequence), axis=1), axis=1)[0]\n",
    "\n",
    "        anomaly_level = classify_anomaly(error)\n",
    "\n",
    "        print(f\"  DevID: {dev_id}, Time: {current_timestamp}, Error: {error:.6f}, Level: {anomaly_level.upper()}\")\n",
    "\n",
    "        device_results['devID'].append(dev_id)\n",
    "        device_results['timestamp'].append(current_timestamp)\n",
    "        device_results['reconstruction_error'].append(error)\n",
    "        device_results['anomaly_level'].append(anomaly_level)\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            if col not in device_results:\n",
    "                device_results[col] = []\n",
    "            device_results[col].append(row[col])\n",
    "\n",
    "print(\"\\n--- Real-time Anomaly Detection Simulation Complete ---\")\n",
    "\n",
    "df_realtime_results = pd.DataFrame(device_results)\n",
    "\n",
    "# --- การพลอตผลลัพธ์ (ถ้ามีข้อมูล) ---\n",
    "if not df_realtime_results.empty:\n",
    "    print(\"\\n--- Summary of Real-time Anomaly Detection Results ---\")\n",
    "    print(df_realtime_results['anomaly_level'].value_counts())\n",
    "\n",
    "    palette_anomaly = {'normal': 'green', 'warning': 'orange', 'critical': 'red'}\n",
    "\n",
    "    # 1. Distribution of Reconstruction Error (Real-time)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_realtime_results['reconstruction_error'], bins=50, kde=True, color='purple', label='Reconstruction Error')\n",
    "    plt.axvline(threshold_warning, color='orange', linestyle='--', label=f'Warning Threshold ({threshold_warning:.5f})')\n",
    "    plt.axvline(threshold_critical, color='red', linestyle='--', label=f'Critical Threshold ({threshold_critical:.5f})')\n",
    "    plt.title('Distribution of Reconstruction Error (Real-time Simulation - Last 1000 Rows)')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Anomaly Level Counts (Real-time)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.countplot(data=df_realtime_results, x='anomaly_level', order=['normal', 'warning', 'critical'], palette=palette_anomaly)\n",
    "    plt.title('Amount of Data in Each Anomaly Level (Real-time Simulation - Last 1000 Rows)')\n",
    "    plt.xlabel('Anomaly Level')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Reconstruction Error Over Time with Anomaly Levels (Real-time)\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.scatterplot(\n",
    "        data=df_realtime_results, x='timestamp', y='reconstruction_error',\n",
    "        hue='devID', # เปลี่ยนเป็น hue='devID' เพื่อดูแยกแต่ละอุปกรณ์\n",
    "        palette='tab10', # ใช้ palette ที่มีสีหลากหลายสำหรับ devID\n",
    "        alpha=0.7, s=15,\n",
    "        style='anomaly_level', # เพิ่ม style เพื่อแยกระดับ Anomaly\n",
    "        markers={'normal': 'o', 'warning': '^', 'critical': 'X'}, # กำหนดรูปร่าง marker\n",
    "        hue_order=sorted(df_realtime_results['devID'].unique()) # เรียง devID ใน legend\n",
    "    )\n",
    "    plt.axhline(threshold_warning, color='orange', linestyle='--', label=f'Warning Threshold ({threshold_warning:.5f})')\n",
    "    plt.axhline(threshold_critical, color='red', linestyle='--', label=f'Critical Threshold ({threshold_critical:.5f})')\n",
    "    plt.title('Reconstruction Error over Time by Device ID (Real-time Simulation - Last 1000 Rows)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Device ID / Anomaly Level', bbox_to_anchor=(1.05, 1), loc='upper left') # ขยับ legend\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Box Plot of Original Features by Anomaly Level (Real-time)\n",
    "    print(\"\\n--- Box Plots of Original Features by Anomaly Level (Real-time Simulation - Last 1000 Rows) ---\")\n",
    "    for feature in numerical_cols:\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        sns.boxplot(data=df_realtime_results, x='anomaly_level', y=feature,\n",
    "                    order=['normal', 'warning', 'critical'], palette=palette_anomaly)\n",
    "        plt.title(f'Boxplot of {feature} by Anomaly Level (Real-time Simulation - Last 1000 Rows)', fontsize=14)\n",
    "        plt.xlabel('Anomaly Level', fontsize=12)\n",
    "        plt.ylabel(feature, fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    print(\"\\nAll Box Plots for Original Features successfully created!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo anomaly detection results were generated for the last 1000 rows. Check data availability and SEQUENCE_LENGTH.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
